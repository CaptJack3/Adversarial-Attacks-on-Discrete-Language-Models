{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ec8a4e5",
   "metadata": {},
   "source": [
    "# Topics in Adversarial Attacks on Deep Learning Models (02360207)\n",
    "#### HW2 - Attacks on Discrete Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31d168",
   "metadata": {},
   "source": [
    "In this HW, we will implement an LLM SOTA attack on a language model trained to perform textual binary classification task. The attack we focus on is Greedy Coordinate Descent (GCG). Before you begin the assignment, it is recommended to read the [original paper of GCG](https://arxiv.org/pdf/2307.15043), understand the mechanisms behind the attack and the logic it implements. This attack is meant to replace continuous optimization, as Language Models operates on discrete inputs (text tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f6950b",
   "metadata": {},
   "source": [
    "<table border=\"1\" cellpadding=\"6\">\n",
    "<tr>\n",
    "<th>Name</th>\n",
    "<th>ID</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Student 1</td>\n",
    "<td>316153261</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Student 2</td>\n",
    "<td>111111111</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Student 3</td>\n",
    "<td>111111111</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5721492-65a8-4b7d-94a9-8f6dddeb713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.3 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 9.9 MB/s  0:00:01\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, pandas\n",
      "\n",
      "   ---------------------------------------- 0/2 [pytz]\n",
      "   ---------------------------------------- 0/2 [pytz]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   -------------------- ------------------- 1/2 [pandas]\n",
      "   ---------------------------------------- 2/2 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829b4dc8-11a3-40d3-9896-649f6e15a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/8.1 MB 7.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.5/8.1 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.1/8.1 MB 14.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 13.6 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 16.5 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ---------------------------------------- 0/6 [pyparsing]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------- ------------------- 3/6 [cycler]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6d6e4d-9acd-421f-a31b-e1346cf71c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1745516f-6044-48c3-b240-1e0e4ecd3dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from requests->transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\master\\.conda\\envs\\adv_env_1\\lib\\site-packages (from requests->transformers) (2025.11.12)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.0 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.0 MB 12.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.0 MB 13.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 13.2 MB/s  0:00:01\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 20.3 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 12.9 MB/s  0:00:00\n",
      "Downloading regex-2025.11.3-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ---------------- ----------------------- 2/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [tokenizers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   -------------------------------- ------- 4/5 [transformers]\n",
      "   ---------------------------------------- 5/5 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951507f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\.conda\\envs\\adv_env_1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2eb704-d88a-485c-a058-aa5fe3003052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5dc22",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f759fc",
   "metadata": {},
   "source": [
    "Our use-case in the assignment is a toxic text classification mission. In the course website, we uploaded a <code>/data</code> directory contains the dataset we work on. In the cell below, we have already included a short script that preprocess the data and sample negative examples from the trainset to balance the training data. Please to do not modify this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1d8a3",
   "metadata": {},
   "source": [
    "<b><span style=\"color: red\">WARNING: The dataset contains offensive language. If you have any problem working on it, please contact me (Omer) via email and I will provide you an alternative dataset to work with.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309bc77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish2\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
    "print(\"Finish2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca007539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[['id', 'comment_text', 'toxic']]\n",
    "negative_sample_train = train_df[train_df['toxic'] == 0].sample(frac=0.1) #downsample the negative class\n",
    "positive_sample_train = train_df[train_df['toxic'] == 1]\n",
    "train_df = pd.concat([negative_sample_train, positive_sample_train])\n",
    "\n",
    "test_labels_df = test_labels_df[['id', 'toxic']]\n",
    "test_df = pd.merge(test_df, test_labels_df, on='id', how='inner')\n",
    "test_df = test_df[test_df['toxic'] != -1]  # remove samples with label `-1`, which indicates **unknown or unlabeled toxicity**\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daa3c5",
   "metadata": {},
   "source": [
    "## Training a text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb80cb3",
   "metadata": {},
   "source": [
    "In this part, you should implement a torch dataset, and train a model of the given architecture to perform the text classification task. Note that the architecture is comprised of a language transformer encoder (recommended for you here to use <code>bert-base-uncased</code>) and a fully connected classification head. We use the transformers library to integrate out transformer. Note also that the paramteres of the transformer are kept frozzen in the architecture (which means we do not fine-tune it, just use the pre-trained embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d0ba6",
   "metadata": {},
   "source": [
    "Complete the dataset class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4133bdd-9425-4d80-bc50-f71c3bccae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Keep raw texts if you need them later (e.g., for running the attack)\n",
    "        self.texts = data[\"comment_text\"].fillna(\"\").astype(str).tolist()\n",
    "        self.labels = data[\"toxic\"].astype(float).tolist()\n",
    "\n",
    "        # Pre-tokenize for speed\n",
    "        enc = self.tokenizer(\n",
    "            self.texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.input_ids = enc[\"input_ids\"]\n",
    "        self.attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            # shape (1,) so DataLoader gives (B,1) which matches sigmoid output\n",
    "            \"labels\": torch.tensor([self.labels[idx]], dtype=torch.float32),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0949ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, freeze_transformer=True):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(transformer_model)\n",
    "\n",
    "        if freeze_transformer:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.get_input_embeddings()\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "        cls_token = self.fc(cls_token)\n",
    "        return self.sigmoid(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897836c2",
   "metadata": {},
   "source": [
    "Define variables for training: device, model, train & test datasets, train & test dataloaders, optimizer and criterion. You can choose hyperparamters on your own, as long as your model gets good accuracy (significantly above random guess) it's alright.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f2c886-583f-4910-b47b-149cd90c7a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n",
      "model is TextClassifier(\n",
      "  (model): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "finish2\n"
     ]
    }
   ],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is {device}\")\n",
    "\n",
    "# model (768-dim encoder)\n",
    "transformer_name = \"distilbert-base-uncased\"   # fast + hidden_size=768\n",
    "model = TextClassifier(transformer_name, freeze_transformer=True).to(device)\n",
    "print(f\"model is {model}\")\n",
    "\n",
    "# datasets (expects columns: comment_text, toxic)\n",
    "train_dataset = TextDataset(train_df, model.tokenizer)\n",
    "test_dataset  = TextDataset(test_df,  model.tokenizer)\n",
    "\n",
    "# loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False)\n",
    "\n",
    "# loss + optimizer (Sigmoid already in model -> BCELoss)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# only train the head (transformer is frozen)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformer_name)\n",
    "\n",
    "print(\"finish2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d3272",
   "metadata": {},
   "source": [
    "Train the model (complete the for-loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9587e6ed-10d8-41a4-b353-1ed45228255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 929/929 [08:40<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.33868546337210065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 929/929 [08:37<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.27838909215383045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 929/929 [09:02<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.2677565708243167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 929/929 [08:43<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.26361326071302393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 929/929 [08:32<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.2608335916839131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)  # shape: (B, 1), float\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)  # shape: (B, 1), sigmoid probs\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{5}, Loss: {epoch_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fc764",
   "metadata": {},
   "source": [
    "Evaluate the model (complete the for-loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aaeb1df-f2e6-4c2d-9c53-020d494399df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 1000/1000 [18:07<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.894494982650286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(test_loader):\n",
    "        # handle either dict-batches (common with tokenizers) or tuple-batches\n",
    "        if isinstance(batch, dict):\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            logits = model(**inputs)\n",
    "        else:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(inputs)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f'Accuracy: {correct/total}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3beee",
   "metadata": {},
   "source": [
    "## Train GCG Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GCGTextClassifierAttack:\n",
    "#     \"\"\"\n",
    "#     Greedy Coordinate Gradient attack for text classification models.\n",
    "#     Generates adversarial suffixes to cause misclassification.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, model, tokenizer, target_class=None, device='cuda'):\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.target_class = target_class\n",
    "#         self.device = device\n",
    "#         self.model.to(device)\n",
    "#         self.model.eval()\n",
    "        \n",
    "#     def compute_loss(self, input_ids, target_class):\n",
    "#         \"\"\"Compute loss for target misclassification\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     def get_token_gradients(self, input_ids, suffix_positions):\n",
    "#         \"\"\"Get gradients with respect to suffix tokens\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     def sample_replacements(self, gradients, current_tokens, k=256, batch_size=512):\n",
    "#         \"\"\"Sample top-k token replacements based on gradients\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     def evaluate_candidates(self, input_ids, suffix_positions, replacements):\n",
    "#         \"\"\"Evaluate loss for candidate token replacements\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     def attack(self, text, suffix_length=10, num_iterations=100, k=256, batch_size=512):\n",
    "#         \"\"\"\n",
    "#         Execute GCG attack to find adversarial suffix\n",
    "        \n",
    "#         Args:\n",
    "#             text: Original text to attack\n",
    "#             suffix_length: Length of adversarial suffix\n",
    "#             num_iterations: Number of optimization iterations\n",
    "#             k: Number of top tokens to consider per position\n",
    "#             batch_size: Number of candidates to evaluate per iteration\n",
    "#         \"\"\"\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40a47e7b-9524-4569-b85e-16ad7d8065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCGTextClassifierAttack:\n",
    "    \"\"\"\n",
    "    Greedy Coordinate Gradient (GCG) attack for text classification.\n",
    "    Learns an adversarial *suffix* (a sequence of tokens appended to the input)\n",
    "    that pushes the model toward a target class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, target_class=None, device='cuda'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_class = target_class\n",
    "        self.device = device\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # internal (set per attack call)\n",
    "        self._current_target_class = None\n",
    "\n",
    "    def _make_attention_mask(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        pad_id = self.tokenizer.pad_token_id\n",
    "        if pad_id is None:\n",
    "            return torch.ones_like(input_ids, dtype=torch.long)\n",
    "        return (input_ids != pad_id).long()\n",
    "\n",
    "    def _forward_probs(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds)\n",
    "        # In this notebook TextClassifier returns sigmoid probs directly: shape (B,1)\n",
    "        return out\n",
    "\n",
    "    def compute_loss(self, input_ids, target_class):\n",
    "        \"\"\"Compute BCE loss toward a target class (0/1).\"\"\"\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = self._make_attention_mask(input_ids).to(self.device)\n",
    "\n",
    "        probs = self._forward_probs(input_ids=input_ids, attention_mask=attention_mask)  # (B,1)\n",
    "        probs = probs.clamp(1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        target = torch.full_like(probs, float(target_class))\n",
    "        loss = F.binary_cross_entropy(probs, target)\n",
    "        return loss\n",
    "\n",
    "    def get_token_gradients(self, input_ids, suffix_positions):\n",
    "        \"\"\"Get gradients w.r.t. *input embeddings* at suffix token positions.\"\"\"\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = self._make_attention_mask(input_ids).to(self.device)\n",
    "\n",
    "        # build differentiable embeddings\n",
    "        emb_layer = self.model.get_input_embeddings()\n",
    "        inputs_embeds = emb_layer(input_ids).detach()\n",
    "        inputs_embeds.requires_grad_(True)\n",
    "\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "\n",
    "        probs = self._forward_probs(inputs_embeds=inputs_embeds, attention_mask=attention_mask)  # (B,1)\n",
    "        probs = probs.clamp(1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        target = torch.full_like(probs, float(self._current_target_class))\n",
    "        loss = F.binary_cross_entropy(probs, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # gradients for suffix positions: (suffix_len, hidden)\n",
    "        grads = inputs_embeds.grad[0, suffix_positions, :].detach().clone()\n",
    "        return grads\n",
    "\n",
    "    def sample_replacements(self, gradients, current_tokens, k=256, batch_size=512):\n",
    "        \"\"\"\n",
    "        Use 1st-order approximation to propose token replacements.\n",
    "        For each suffix position i: choose tokens that most reduce loss ~ minimize grad_i · emb(token).\n",
    "        \"\"\"\n",
    "        emb_weight = self.model.get_input_embeddings().weight  # (V, H)\n",
    "        # scores: higher is better improvement (we take -grad·emb)\n",
    "        scores = -torch.matmul(gradients, emb_weight.t())  # (suffix_len, V)\n",
    "\n",
    "        # avoid special tokens + keep text readable\n",
    "        if hasattr(self.tokenizer, \"all_special_ids\") and self.tokenizer.all_special_ids:\n",
    "            scores[:, self.tokenizer.all_special_ids] = -float(\"inf\")\n",
    "\n",
    "        # avoid keeping the same token\n",
    "        pos_idx = torch.arange(scores.size(0), device=scores.device)\n",
    "        scores[pos_idx, current_tokens] = -float(\"inf\")\n",
    "\n",
    "        # top-k per position\n",
    "        topk = torch.topk(scores, k=min(k, scores.size(1)), dim=1)\n",
    "        topk_ids = topk.indices  # (suffix_len, k)\n",
    "        topk_scores = topk.values\n",
    "\n",
    "        # flatten (position, token) candidates, sort by heuristic score, keep up to batch_size\n",
    "        candidates = []\n",
    "        suffix_len = topk_ids.size(0)\n",
    "        kk = topk_ids.size(1)\n",
    "        for i in range(suffix_len):\n",
    "            for j in range(kk):\n",
    "                candidates.append((i, int(topk_ids[i, j].item()), float(topk_scores[i, j].item())))\n",
    "\n",
    "        candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "        candidates = candidates[:batch_size]\n",
    "        # return (suffix_pos_index, token_id)\n",
    "        return [(i, tok) for (i, tok, _) in candidates]\n",
    "\n",
    "    def evaluate_candidates(self, input_ids, suffix_positions, replacements):\n",
    "        \"\"\"Evaluate true loss for each candidate replacement; return best (pos_idx, token_id, loss).\"\"\"\n",
    "        if len(replacements) == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        base_ids = input_ids.to(self.device)\n",
    "        N = len(replacements)\n",
    "\n",
    "        # build a batch of candidates\n",
    "        cand_ids = base_ids.repeat(N, 1)\n",
    "        for n, (pos_in_suffix, tok_id) in enumerate(replacements):\n",
    "            abs_pos = suffix_positions[pos_in_suffix]\n",
    "            cand_ids[n, abs_pos] = tok_id\n",
    "\n",
    "        # evaluate losses\n",
    "        losses = []\n",
    "        bs = 512  # internal eval batch (can be same as your batch_size param)\n",
    "        for start in range(0, N, bs):\n",
    "            chunk = cand_ids[start:start + bs]\n",
    "            mask = self._make_attention_mask(chunk).to(self.device)\n",
    "\n",
    "            probs = self._forward_probs(input_ids=chunk, attention_mask=mask)  # (B,1)\n",
    "            probs = probs.clamp(1e-7, 1.0 - 1e-7)\n",
    "\n",
    "            target = torch.full_like(probs, float(self._current_target_class))\n",
    "            # per-sample loss\n",
    "            chunk_loss = F.binary_cross_entropy(probs, target, reduction='none').mean(dim=1)  # (B,)\n",
    "            losses.append(chunk_loss.detach())\n",
    "\n",
    "        losses = torch.cat(losses, dim=0)  # (N,)\n",
    "        best_idx = int(torch.argmin(losses).item())\n",
    "        best_pos, best_tok = replacements[best_idx]\n",
    "        best_loss = float(losses[best_idx].item())\n",
    "        return best_pos, best_tok, best_loss\n",
    "\n",
    "    def attack(self, text, suffix_length=10, num_iterations=100, k=256, batch_size=512):\n",
    "        \"\"\"\n",
    "        Execute GCG attack to find adversarial suffix.\n",
    "\n",
    "        Returns:\n",
    "            adv_text: decoded full adversarial text\n",
    "            adv_suffix: decoded adversarial suffix only\n",
    "            info: dict with details\n",
    "        \"\"\"\n",
    "        # Tokenize (leave room for suffix)\n",
    "        max_len = getattr(self.tokenizer, \"model_max_length\", 512)\n",
    "        max_len = 512 if (max_len is None or max_len > 10_000) else max_len\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max(8, max_len - suffix_length),\n",
    "            padding=False,\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].to(self.device)\n",
    "\n",
    "        # pick target class\n",
    "        with torch.no_grad():\n",
    "            mask = self._make_attention_mask(input_ids).to(self.device)\n",
    "            p = self._forward_probs(input_ids=input_ids, attention_mask=mask)  # (1,1)\n",
    "            pred = int((p.item() > 0.5))\n",
    "\n",
    "        target_class = self.target_class\n",
    "        if target_class is None:\n",
    "            target_class = 1 - pred  # flip for binary\n",
    "        self._current_target_class = target_class\n",
    "\n",
    "        # init suffix tokens\n",
    "        init_tok = (\n",
    "            self.tokenizer.mask_token_id\n",
    "            if self.tokenizer.mask_token_id is not None\n",
    "            else (self.tokenizer.unk_token_id if self.tokenizer.unk_token_id is not None else 0)\n",
    "        )\n",
    "        suffix = torch.full((1, suffix_length), init_tok, dtype=torch.long, device=self.device)\n",
    "\n",
    "        # append suffix before [SEP] if present at the end, otherwise append at end\n",
    "        sep_id = self.tokenizer.sep_token_id\n",
    "        if sep_id is not None and input_ids.size(1) > 0 and int(input_ids[0, -1].item()) == int(sep_id):\n",
    "            prefix = input_ids[:, :-1]\n",
    "            sep = input_ids[:, -1:]\n",
    "            adv_ids = torch.cat([prefix, suffix, sep], dim=1)\n",
    "            start_pos = prefix.size(1)\n",
    "        else:\n",
    "            adv_ids = torch.cat([input_ids, suffix], dim=1)\n",
    "            start_pos = input_ids.size(1)\n",
    "\n",
    "        suffix_positions = list(range(start_pos, start_pos + suffix_length))\n",
    "\n",
    "        # initial loss\n",
    "        current_loss = float(self.compute_loss(adv_ids, target_class).item())\n",
    "        best_pred = pred\n",
    "\n",
    "        for it in range(num_iterations):\n",
    "            grads = self.get_token_gradients(adv_ids, suffix_positions)  # (suffix_len, H)\n",
    "            current_suffix_tokens = adv_ids[0, suffix_positions].detach()\n",
    "\n",
    "            replacements = self.sample_replacements(\n",
    "                gradients=grads,\n",
    "                current_tokens=current_suffix_tokens,\n",
    "                k=k,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "            best_pos, best_tok, best_loss = self.evaluate_candidates(adv_ids, suffix_positions, replacements)\n",
    "            if best_pos is None:\n",
    "                break\n",
    "\n",
    "            # stop if no improvement\n",
    "            if best_loss >= current_loss - 1e-6:\n",
    "                break\n",
    "\n",
    "            # apply greedy update\n",
    "            adv_ids[0, suffix_positions[best_pos]] = best_tok\n",
    "            current_loss = best_loss\n",
    "\n",
    "            # check success (hit target)\n",
    "            with torch.no_grad():\n",
    "                mask = self._make_attention_mask(adv_ids).to(self.device)\n",
    "                p = self._forward_probs(input_ids=adv_ids, attention_mask=mask)\n",
    "                best_pred = int((p.item() > 0.5))\n",
    "                if best_pred == target_class:\n",
    "                    break\n",
    "\n",
    "        adv_suffix_ids = adv_ids[0, suffix_positions].tolist()\n",
    "        adv_suffix = self.tokenizer.decode(adv_suffix_ids, skip_special_tokens=True)\n",
    "        adv_text = self.tokenizer.decode(adv_ids[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "        info = {\n",
    "            \"orig_pred\": pred,\n",
    "            \"target_class\": target_class,\n",
    "            \"final_pred\": best_pred,\n",
    "            \"final_loss\": current_loss,\n",
    "            \"num_iterations_ran\": it + 1,\n",
    "        }\n",
    "        return adv_text, adv_suffix, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80404098",
   "metadata": {},
   "source": [
    "Run the attack on 10 samples from the test set that your model predicts correctly. Show that the attack manages to cause misclassification. Print in the following format: \n",
    "* original sample: <code>your original toxic text</code>\n",
    "* original logits: probability of the model to input being toxic\n",
    "* attacked sample: <code>your original toxic text after attack</code>\n",
    "* attacked logits: probability of the model to input after attack being toxic \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass # your code for running the attack on test samples and reporting results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928fde72-5409-4904-aeb3-b6266c9de2f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'GCGTextClassifierAttack' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 108\u001b[0m, in \u001b[0;36mrun_attack_on_10_correct\u001b[1;34m(model, tokenizer, test_dataset, attack, device, threshold, max_tries)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     attacked_text \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(text, target_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_true)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# fallback: attack might not take target_class\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GCGTextClassifierAttack' object has no attribute 'generate'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[0;32m    137\u001b[0m attack \u001b[38;5;241m=\u001b[39m GCGTextClassifierAttack(model, tokenizer, target_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(device))\n\u001b[1;32m--> 138\u001b[0m \u001b[43mrun_attack_on_10_correct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 114\u001b[0m, in \u001b[0;36mrun_attack_on_10_correct\u001b[1;34m(model, tokenizer, test_dataset, attack, device, threshold, max_tries)\u001b[0m\n\u001b[0;32m    111\u001b[0m     attacked_text \u001b[38;5;241m=\u001b[39m attack\u001b[38;5;241m.\u001b[39mgenerate(text)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# fallback: attack might be callable\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     attacked_text \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    117\u001b[0m p_att \u001b[38;5;241m=\u001b[39m toxic_probability_from_model(model, tokenizer, attacked_text, device)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'GCGTextClassifierAttack' object is not callable"
     ]
    }
   ],
   "source": [
    "# def toxic_probability_from_model(model, tokenizer, text, device, toxic_class_idx=1):\n",
    "#     \"\"\"\n",
    "#     Returns P(toxic) as float.\n",
    "#     Works whether model returns:\n",
    "#       - a Tensor logits\n",
    "#       - a tuple (logits, ...)\n",
    "#       - a dict with 'logits'\n",
    "#       - a HF ModelOutput with .logits\n",
    "#     \"\"\"\n",
    "#     enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#     enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "#     out = model(**enc)\n",
    "\n",
    "#     # --- extract logits ---\n",
    "#     if torch.is_tensor(out):\n",
    "#         logits = out\n",
    "#     elif isinstance(out, (tuple, list)):\n",
    "#         logits = out[0]\n",
    "#     elif isinstance(out, dict) and \"logits\" in out:\n",
    "#         logits = out[\"logits\"]\n",
    "#     else:\n",
    "#         logits = out.logits  # HF-style\n",
    "\n",
    "#     # ensure shape [batch, C] or [batch, 1]\n",
    "#     if logits.dim() == 1:\n",
    "#         logits = logits.unsqueeze(0)\n",
    "\n",
    "#     # --- convert to probability of toxic ---\n",
    "#     if logits.size(-1) == 1:\n",
    "#         # single-logit binary classifier: sigmoid(logit)\n",
    "#         p_toxic = torch.sigmoid(logits.squeeze(0).squeeze(-1))\n",
    "#     else:\n",
    "#         # multi-logit classifier: softmax, take toxic_class_idx (usually 1)\n",
    "#         probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "#         p_toxic = probs[toxic_class_idx]\n",
    "\n",
    "#     return float(p_toxic.detach().cpu())\n",
    "\n",
    "\n",
    "# def pred_label_from_prob(p_toxic, threshold=0.5):\n",
    "#     return 1 if p_toxic >= threshold else 0\n",
    "\n",
    "# def extract_text_and_label(sample, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Tries to extract (text, label) from different dataset formats.\n",
    "#     If the dataset stores tokenized inputs, we decode them back to text.\n",
    "#     \"\"\"\n",
    "#     # Case A: sample is dict\n",
    "#     if isinstance(sample, dict):\n",
    "#         label = sample.get(\"label\", sample.get(\"labels\", None))\n",
    "#         if torch.is_tensor(label):\n",
    "#             label = int(label.item())\n",
    "\n",
    "#         if \"text\" in sample:\n",
    "#             text = sample[\"text\"]\n",
    "#             return text, label\n",
    "\n",
    "#         if \"input_ids\" in sample:\n",
    "#             ids = sample[\"input_ids\"]\n",
    "#             if torch.is_tensor(ids):\n",
    "#                 ids = ids.tolist()\n",
    "#             text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "#             return text, label\n",
    "\n",
    "#     # Case B: sample is tuple (text, label) or (enc, label)\n",
    "#     if isinstance(sample, (tuple, list)) and len(sample) >= 2:\n",
    "#         a, b = sample[0], sample[1]\n",
    "#         # if first element looks like raw text\n",
    "#         if isinstance(a, str):\n",
    "#             text = a\n",
    "#             label = int(b.item()) if torch.is_tensor(b) else int(b)\n",
    "#             return text, label\n",
    "#         # if first element is token ids tensor/list\n",
    "#         if torch.is_tensor(a) or isinstance(a, (list, tuple)):\n",
    "#             ids = a.tolist() if torch.is_tensor(a) else a\n",
    "#             text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "#             label = int(b.item()) if torch.is_tensor(b) else int(b)\n",
    "#             return text, label\n",
    "\n",
    "#     raise ValueError(\"Couldn't parse sample format. Add a custom extractor for your dataset.\")\n",
    "\n",
    "# def run_attack_on_10_correct(model, tokenizer, test_dataset, attack, device=\"cuda\", threshold=0.5, max_tries=500):\n",
    "#     model.eval()\n",
    "#     device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.to(device)\n",
    "\n",
    "#     found = 0\n",
    "#     i = 0\n",
    "#     while found < 10 and i < len(test_dataset) and max_tries > 0:\n",
    "#         max_tries -= 1\n",
    "#         sample = test_dataset[i]\n",
    "#         i += 1\n",
    "\n",
    "#         text, y_true = extract_text_and_label(sample, tokenizer)\n",
    "\n",
    "#         # Compute original toxic probability + predicted label\n",
    "#         p_orig = toxic_probability_from_model(model, tokenizer, text, device)\n",
    "#         y_pred = pred_label_from_prob(p_orig, threshold)\n",
    "\n",
    "#         # Keep only correctly predicted samples\n",
    "#         if y_pred != y_true:\n",
    "#             continue\n",
    "\n",
    "#         # ---- Run attack (change ONLY this line if your API differs) ----\n",
    "#         # Example expected: attacked_text = attack.generate(text, target_class=1-y_true) OR attack(text)\n",
    "#         try:\n",
    "#             attacked_text = attack.generate(text, target_class=1 - y_true)\n",
    "#         except TypeError:\n",
    "#             # fallback: attack might not take target_class\n",
    "#             attacked_text = attack.generate(text)\n",
    "#         except AttributeError:\n",
    "#             # fallback: attack might be callable\n",
    "#             attacked_text = attack(text)\n",
    "#         # ---------------------------------------------------------------\n",
    "\n",
    "#         p_att = toxic_probability_from_model(model, tokenizer, attacked_text, device)\n",
    "#         y_att = pred_label_from_prob(p_att, threshold)\n",
    "\n",
    "#         # We want misclassification (flip)\n",
    "#         if y_att == y_true:\n",
    "#             continue\n",
    "\n",
    "#         found += 1\n",
    "\n",
    "#         print(f\"* original sample: <code>{text}</code>\")\n",
    "#         print(f\"* original logits: {p_orig}\")\n",
    "#         print(f\"* attacked sample: <code>{attacked_text}</code>\")\n",
    "#         print(f\"* attacked logits: {p_att}\")\n",
    "#         print()\n",
    "\n",
    "#     if found < 10:\n",
    "#         print(f\"Only found {found} successful attacks (misclassifications). \"\n",
    "#               f\"Try increasing attack strength/steps or search more samples.\")\n",
    "\n",
    "# # Usage:\n",
    "# attack = GCGTextClassifierAttack(model, tokenizer, target_class=None, device=str(device))\n",
    "# run_attack_on_10_correct(model, tokenizer, test_dataset, attack, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a83582c7-f26c-4806-aa7a-51abb7868bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* original sample: <code>Thank you for understanding. I think very highly of you and would not revert without discussion.</code>\n",
      "* original logits: 0.01077330019325018\n",
      "* attacked sample: <code>thank you for understanding. i think very highly of you and would not revert without discussion. bastard</code>\n",
      "* attacked logits: 0.5732153058052063\n",
      "\n",
      "* original sample: <code>== Arabs are committing genocide in Iraq, but no protests in Europe. == \n",
      "\n",
      " May Europe also burn in hell.</code>\n",
      "* original logits: 0.849972665309906\n",
      "* attacked sample: <code>= = arabs are committing genocide in iraq, but no protests in europe. = = may europe also burn in hell. highlighted tentative</code>\n",
      "* attacked logits: 0.11034949868917465\n",
      "\n",
      "* original sample: <code>Professors to the Manhatten Project.</code>\n",
      "* original logits: 0.02696494571864605\n",
      "* attacked sample: <code>professors to the manhatten project. fucking</code>\n",
      "* attacked logits: 0.7693648338317871\n",
      "\n",
      "* original sample: <code>일이삼사오육칠팔구하고십이요 에헤헤 으헤 으헤 으허허</code>\n",
      "* original logits: 0.060370877385139465\n",
      "* attacked sample: <code>일이삼사오육칠팔구하고십이요 에헤헤 으헤 으헤 으허허 sex pig</code>\n",
      "* attacked logits: 0.6649594306945801\n",
      "\n",
      "* original sample: <code>\" \n",
      " I'm not convinced that he was blind. Where is this documented? It's possible that he was just what we'd call \"\"Legally blind\"\" ie didn't have great vision, and that the name \"\"Blind Blake\"\" is an exaggerated moniker. Although I have no proof I've got a feeling that him being legally blind is more likely than totally blind. Of course I've got no evidence to back that thing up.\"</code>\n",
      "* original logits: 0.11198416352272034\n",
      "* attacked sample: <code>\" i ' m not convinced that he was blind. where is this documented? it ' s possible that he was just what we ' d call \" \" legally blind \" \" ie didn ' t have great vision, and that the name \" \" blind blake \" \" is an exaggerated moniker. although i have no proof i ' ve got a feeling that him being legally blind is more likely than totally blind. of course i ' ve got no evidence to back that thing up. \" bitch</code>\n",
      "* attacked logits: 0.8040748238563538\n",
      "\n",
      "* original sample: <code>I'd never think I'd need to say it, but Wikipedia isn't a fansite discussion board. If anything is unannounced by any authority, it might as well be false. MMORPGs are overrated,</code>\n",
      "* original logits: 0.21229287981987\n",
      "* attacked sample: <code>i ' d never think i ' d need to say it, but wikipedia isn ' t a fansite discussion board. if anything is unannounced by any authority, it might as well be false. mmorpgs are overrated, fucking</code>\n",
      "* attacked logits: 0.7658273577690125\n",
      "\n",
      "* original sample: <code>DJ Robinson is gay as hell! he sucks his dick so much!!!!!</code>\n",
      "* original logits: 0.9962652325630188\n",
      "* attacked sample: <code>dj robinson is gay as hell! he sucks his dick so much!!!!! orchestra signifies composer</code>\n",
      "* attacked logits: 0.49680641293525696\n",
      "\n",
      "* original sample: <code>:Fuck off, you anti-semitic cunt.  |</code>\n",
      "* original logits: 0.9980267882347107\n",
      "* attacked sample: <code>: fuck off, you anti - semitic cunt. | gma music primetime</code>\n",
      "* attacked logits: 0.2992783784866333\n",
      "\n",
      "* original sample: <code>Puwersa ng Masa!</code>\n",
      "* original logits: 0.2408268004655838\n",
      "* attacked sample: <code>puwersa ng masa! fuck</code>\n",
      "* attacked logits: 0.9099838733673096\n",
      "\n",
      "* original sample: <code>REDIRECT Talk:Kemp's thicket rat</code>\n",
      "* original logits: 0.26478689908981323\n",
      "* attacked sample: <code>redirect talk : kemp ' s thicket rat fucking</code>\n",
      "* attacked logits: 0.7709194421768188\n",
      "\n",
      "Successful misclassifications: 10/10 (searched through 28 correctly-predicted samples)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def p_toxic(model, tokenizer, text, device):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        prob = model(**enc)  # your TextClassifier returns sigmoid prob (B,1)\n",
    "        return float(prob.view(-1)[0].item())\n",
    "\n",
    "def pred_from_p(p):\n",
    "    return 1 if p >= 0.5 else 0\n",
    "\n",
    "def run_attack_on_10_correct(model, tokenizer, test_dataset, attack, device=\"cuda\",\n",
    "                             suffix_length=10, num_iterations=100, k=256, batch_size=512):\n",
    "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device).eval()\n",
    "\n",
    "    successes = 0\n",
    "    tried = 0\n",
    "\n",
    "    # go through the test set until we collect 10 *successful* misclassifications\n",
    "    for i in range(len(test_dataset)):\n",
    "        # adapt to your dataset structure (common in this HW: dataset.texts / dataset.labels)\n",
    "        text = test_dataset.texts[i]\n",
    "        y_true = int(test_dataset.labels[i])\n",
    "\n",
    "        p_orig = p_toxic(model, tokenizer, text, device)\n",
    "        y_pred = pred_from_p(p_orig)\n",
    "\n",
    "        # only take samples predicted correctly\n",
    "        if y_pred != y_true:\n",
    "            continue\n",
    "\n",
    "        tried += 1\n",
    "\n",
    "        adv_text, adv_suffix, info = attack.attack(\n",
    "            text,\n",
    "            suffix_length=suffix_length,\n",
    "            num_iterations=num_iterations,\n",
    "            k=k,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        p_att = p_toxic(model, tokenizer, adv_text, device)\n",
    "        y_att = pred_from_p(p_att)\n",
    "\n",
    "        # success = now misclassified (since original was correct)\n",
    "        if y_att == y_true:\n",
    "            continue\n",
    "\n",
    "        successes += 1\n",
    "\n",
    "        print(f\"* original sample: <code>{text}</code>\")\n",
    "        print(f\"* original logits: {p_orig}\")\n",
    "        print(f\"* attacked sample: <code>{adv_text}</code>\")\n",
    "        print(f\"* attacked logits: {p_att}\")\n",
    "        print()\n",
    "\n",
    "        if successes >= 10:\n",
    "            break\n",
    "\n",
    "    print(f\"Successful misclassifications: {successes}/10 (searched through {tried} correctly-predicted samples)\")\n",
    "\n",
    "# usage\n",
    "attack = GCGTextClassifierAttack(model, tokenizer, target_class=None, device=\"cuda\")\n",
    "run_attack_on_10_correct(model, tokenizer, test_dataset, attack, device=\"cuda\")\n",
    "print(\"Finish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
